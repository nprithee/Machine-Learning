{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Import libaries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","from xgboost import XGBRegressor\n","from lightgbm import LGBMRegressor\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import tensorflow as tf\n","tf.random.set_seed(42)\n","np.random.seed(42)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WE2Pzfd0rtko"},"outputs":[],"source":["from google.colab import drive\n","import sys\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Get the absolute path of the current folder\n","abspath_curr = '/content/drive/My Drive/Colab Notebooks/final_project/'"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ZOp1r3fdsqH_"},"outputs":[],"source":["import warnings\n","\n","# Ignore warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"l7KIqeQssrI5"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline \n","\n","# Set matplotlib sizes\n","plt.rc('font', size=20)\n","plt.rc('axes', titlesize=20)\n","plt.rc('axes', labelsize=20)\n","plt.rc('xtick', labelsize=20)\n","plt.rc('ytick', labelsize=20)\n","plt.rc('legend', fontsize=20)\n","plt.rc('figure', titlesize=20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S1yNlBJsst7g"},"outputs":[],"source":["# The magic below allows us to use tensorflow version 2.x\n","%tensorflow_version 2.x \n","import tensorflow as tf\n","from tensorflow import keras"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mJE1B5QxsyhL"},"outputs":[],"source":["# The random seed\n","random_seed = 42\n","\n","# Set random seed in tensorflow\n","tf.random.set_seed(random_seed)\n","\n","# Set random seed in numpy\n","import numpy as np\n","np.random.seed(random_seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ppl0XugIszRQ"},"outputs":[],"source":["# Implement me\n","\n","# Loading raw training data\n","df_raw_train = pd.read_csv(abspath_curr + 'data/DailyDelhiClimateTrain.csv',header=0)\n","\n","# Make a copy of the raw training data\n","df_train = df_raw_train.copy(deep=True)\n","\n","# Loading raw test data\n","df_raw_test = pd.read_csv(abspath_curr + 'data/DailyDelhiClimateTest.csv',header=0)\n","\n","# Make a copy of the raw test data\n","df_test = df_raw_test.copy(deep=True)\n","\n","# Name of target\n","target=\"trip_duration\""]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##Exploratory Data Analysis (EDA)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check the dimensions of the training and test datasets\n","print(\"Training data shape:\", df_train.shape)\n","print(\"Test data shape:\", df_test.shape)\n","\n","# Get the summary statistics of the training dataset\n","print(df_train.describe())\n","\n","# Check for missing values in the training and test datasets\n","print(\"Missing values in training data:\\n\", df_train.isnull().sum())\n","print(\"Missing values in test data:\\n\", df_test.isnull().sum())\n","\n","# Visualize the distribution of the target variable\n","sns.histplot(data=df_train, x=target, bins=30)\n","\n","# Visualize the correlations between the features and the target variable\n","sns.pairplot(data=df_train, x_vars=['feature1', 'feature2', ...], y_vars=[target], kind='scatter')\n","\n","# Plot the time series of the target variable\n","df_train.set_index('date', inplace=True)\n","df_train[target].plot(figsize=(15, 5))\n","\n","# Identify outliers using box plots or scatter plots\n","sns.boxplot(data=df_train, x=target)\n","sns.scatterplot(data=df_train, x='feature1', y=target)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Data Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","# Impute missing values using mean, median, or mode\n","df_train.fillna(df_train.mean(), inplace=True)\n","df_test.fillna(df_test.mean(), inplace=True)\n","\n","# One-hot encode categorical variables\n","df_train = pd.get_dummies(df_train, columns=['categorical_column'])\n","df_test = pd.get_dummies(df_test, columns=['categorical_column'])\n","\n","# Scale the features using standardization or normalization\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(df_train.drop([target], axis=1))\n","y_train = df_train[target]\n","X_test = scaler.transform(df_test.drop([target], axis=1))\n","y_test = df_test[target]\n","\n","# Feature engineering using domain knowledge\n","df_train['new_feature'] = df_train['feature1'] + df_train['feature2']\n","df_test['new_feature'] = df_test['feature1'] + df_test['feature2']\n","\n","# Split the data into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=random_seed)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Hyperparameter Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the hyperparameters and their search spaces\n","param_grid = {\n","    'n_estimators': [100, 200, 300],\n","    'max_depth': [3, 4, 5],\n","    'min_samples_split': [2, 3, 4],\n","    'min_samples_leaf': [1, 2, 3]\n","}\n","\n","# Initialize the grid search with the estimator, hyperparameters, and scoring metric\n","rf_grid = GridSearchCV(RandomForestRegressor(random_state=random_seed), \n","                       param_grid, \n","                       scoring='neg_mean_squared_error',\n","                       cv=5, \n","                       n_jobs=-1)\n","\n","# Fit the grid search to the training data\n","rf_grid.fit(X_train, y_train)\n","\n","# Get the best hyperparameters and their performance score\n","best_params = rf_grid.best_params_\n","best_score = rf_grid.best_score_\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Model Selection"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize the models\n","models = {\n","    'linear_regression': LinearRegression(),\n","    'ridge_regression': Ridge(),\n","    'lasso_regression': Lasso(),\n","    'elastic_net': ElasticNet(),\n","    'random_forest': RandomForestRegressor(random_state=random_seed),\n","    'gradient_boosting': GradientBoostingRegressor(random_state=random_seed),\n","    'xgboost': XGBRegressor(random_state=random_seed),\n","    'lightgbm': LGBMRegressor(random_state=random_seed)\n","}\n","\n","# Define the hyperparameters and their search spaces for each model\n","param_grids = {\n","    'linear_regression': {},\n","    'ridge_regression': {'alpha': [0.1, 1.0, 10.0]},\n","    'lasso_regression': {'alpha': [0.1, 1.0, 10.0]},\n","    'elastic_net': {'alpha': [0.1, 1.0, 10.0], 'l1_ratio': [0.1, 0.5, 0.9]},\n","    'random_forest': {'n_estimators': [100, 200, 300], 'max_depth': [3, 4, 5], \n","                      'min_samples_split': [2, 3, 4], 'min_samples_leaf': [1, 2, 3]},\n","    'gradient_boosting': {'n_estimators': [100, 200, 300], 'max_depth': [3, 4, 5], \n","                          'learning_rate': [0.1, 0.01, 0.001]},\n","    'xgboost': {'n_estimators': [100, 200, 300], 'max_depth': [3, 4, 5], \n","                'learning_rate': [0.1, 0.01, 0.001]},\n","    'lightgbm': {'n_estimators': [100, 200, 300], 'max_depth': [3, 4, 5], \n","                 'learning_rate': [0.1, 0.01, 0.001]}\n","}\n","\n","# Initialize the results dictionary\n","results = {}\n","\n","# Loop over each model and perform cross-validation with hyperparameter tuning\n","for model_name, model in models.items():\n","    print(\"Training\", model_name)\n","    \n","    # Define the grid search with the estimator, hyperparameters, and scoring metric\n","    grid = GridSearchCV(model, param_grids[model_name], scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n","    \n","    # Fit the grid search to the training data\n","    grid.fit(X_train, y_train)\n","    \n","    # Get the best hyperparameters and their performance score\n","    best_params = grid.best_params_\n","    best_score = grid.best_score_\n","    \n","    # Evaluate the model on the validation set\n","    y_pred = grid.predict(X_val)\n","    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n","    r2 = r2_score(y_val, y_pred)\n","    \n","    # Save the results to the dictionary\n","    results[model_name] = {'best_params': best_params, 'best_score': best_score, \n","                           'rmse': rmse, 'r2': r2}\n","\n","# Print the results dictionary\n","print(results)\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNBkcW7iwIEOS0n+u9pHHkA","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
